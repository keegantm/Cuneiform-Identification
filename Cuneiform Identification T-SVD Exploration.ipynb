{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # tfidf weighting\n",
    "import re\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score,f1_score, make_scorer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import LocallyLinearEmbedding, TSNE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "'''nltk.download('stopwords')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('punkt')'''\n",
    "\n",
    "'''%cd \"/Users/keeganmoseley/Desktop/Roux/CS6140 - Machine Learning/Final Project\"\n",
    "os.listdir()'''\n",
    "cuneiform_whole = pd.read_csv(\"train.csv\")\n",
    "cuneiform_df = cuneiform_whole.sample(frac=0.25, random_state=251)\n",
    "cuneiform_df.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_assessment(cuneiform_df, vectorizer, pca):\n",
    "    #split the data into training and testing sets\n",
    "    X = cuneiform_df['cuneiform']\n",
    "    y = cuneiform_df['lang']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=251)\n",
    "\n",
    "    train_df = pd.DataFrame({'cuneiform': X_train, 'lang': y_train})\n",
    "    test_df = pd.DataFrame({'cuneiform': X_test, 'lang': y_test})\n",
    "    train_df.sort_values(by='lang', inplace=True)\n",
    "    test_df.sort_values(by='lang', inplace=True)\n",
    "\n",
    "    train_df.reset_index(drop=True, inplace = True)\n",
    "    test_df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "    print(\"Data has been split!\")\n",
    "\n",
    "    #-----------------------Create a DTM---------------------------------------------\n",
    "\n",
    "    #train the model\n",
    "    tfidf_model = vectorizer.fit(train_df[\"cuneiform\"])\n",
    "\n",
    "    #apply it to the training data\n",
    "    tfidf_matrix_train = vectorizer.transform(train_df[\"cuneiform\"])\n",
    "\n",
    "    #turn results into a document term matrix\n",
    "    dense_matrix_train = tfidf_matrix_train.todense()   \n",
    "\n",
    "    # Retrieve feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names_out() \n",
    "\n",
    "    # Create a DataFrame with the dense matrix and feature names as columns\n",
    "    tfidf_df_train = pd.DataFrame(dense_matrix_train, columns=feature_names)\n",
    "\n",
    "    #Create document term matrix of Test data\n",
    "    tfidf_matrix_test = vectorizer.transform(test_df[\"cuneiform\"])\n",
    "\n",
    "    #turn results into a document term matrix\n",
    "    dense_matrix_test = tfidf_matrix_test.todense()   \n",
    "\n",
    "    # Retrieve feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names_out() \n",
    "\n",
    "    # Create a DataFrame with the dense matrix and feature names as columns\n",
    "    tfidf_df_test = pd.DataFrame(dense_matrix_test, columns=feature_names)\n",
    "\n",
    "    #----------------------- Preprocess and Standardize the Data ----------------------\n",
    "\n",
    "    #Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(tfidf_df_train)\n",
    "    training_df_standardized = pd.DataFrame(scaler.transform(tfidf_df_train), columns=tfidf_df_train.columns) #dataframe of standardized data\n",
    "    test_df_standardized = pd.DataFrame(scaler.transform(tfidf_df_test), columns=tfidf_df_test.columns)\n",
    "\n",
    "    #find local outliers with LOF\n",
    "    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)\n",
    "    # predict the labels for each data point (as Outlier or inlier)\n",
    "    pred_lof = lof.fit_predict(training_df_standardized)\n",
    "\n",
    "    #find outliers with isolation forrest\n",
    "    iforest = IsolationForest(n_estimators=100,  contamination=0.01)\n",
    "    # predict the labels for each data point (as Outlier or inlier)\n",
    "    pred_if = iforest.fit_predict(training_df_standardized)\n",
    "\n",
    "    #Add columns of the outlier results\n",
    "    training_df_standardized[\"pred_lof\"] = pred_lof\n",
    "    training_df_standardized[\"pred_if\"] = pred_if\n",
    "\n",
    "    '''#find how many outliers there are\n",
    "    lof_count = (training_df_standardized[\"pred_lof\"] == -1).sum()\n",
    "    if_count = (training_df_standardized[\"pred_if\"] == -1).sum()\n",
    "    total_count = ((training_df_standardized[\"pred_lof\"] == -1) & (training_df_standardized[\"pred_if\"] == -1)).sum()\n",
    "\n",
    "    print(\"Outliers found by Local Outlier Factor :\", lof_count)\n",
    "    print(\"Outliers found by Isolation Forrest : \",if_count)\n",
    "    print(\"Outliers Tagged by Both : \",total_count)\n",
    "\n",
    "    percentage_outliers = (lof_count + if_count)/(training_df_standardized.shape[0])\n",
    "    print(percentage_outliers * 100,'% of the training data are outliers')'''\n",
    "\n",
    "    #remove the outliers\n",
    "    outliers = training_df_standardized[(training_df_standardized[\"pred_lof\"] == -1) | (training_df_standardized[\"pred_if\"] == -1)][[\"pred_lof\", \"pred_if\"]]\n",
    "    #print(\"Outliers Found By Either Algorithm :\")\n",
    "    #display(outliers)\n",
    "\n",
    "    #remove outliers from dataframe\n",
    "    outlier_indices = outliers.index\n",
    "    training_df_standardized.drop(outlier_indices, axis=0)\n",
    "    train_df.drop(outlier_indices, axis= 0)\n",
    "\n",
    "\n",
    "    # ---------------------------- Dimension Reduction -------------------------\n",
    "    x_subset_train = training_df_standardized.drop(['pred_lof', 'pred_if'], axis=1).values\n",
    "    y_subset_train = train_df[\"lang\"].values\n",
    "\n",
    "    x_subset_test = test_df_standardized.values\n",
    "    y_subset_test = test_df[\"lang\"].values\n",
    "\n",
    "    pca_trained = pca.fit(x_subset_train)\n",
    "    pca_train_results = pca_trained.transform(x_subset_train)\n",
    "    pca_test_results = pca_trained.transform(x_subset_test)\n",
    "\n",
    "    #evaluate the pc variance captured\n",
    "    explained_variance_ratio = pca_trained.explained_variance_ratio_\n",
    "    variance_df = pd.DataFrame(data={\n",
    "        'Principal Component': [f'PC{i+1}' for i in range(len(explained_variance_ratio))],\n",
    "        'Explained Variance' : explained_variance_ratio,\n",
    "        'Cumulative Variance' : explained_variance_ratio.cumsum()\n",
    "    })\n",
    "\n",
    "    return [pca_train_results, pca_test_results, y_subset_train, y_subset_test, variance_df]\n",
    "\n",
    "def pca_no_outliers_removed(cuneiform_df, vectorizer, pca):\n",
    "    #split the data into training and testing sets\n",
    "    X = cuneiform_df['cuneiform']\n",
    "    y = cuneiform_df['lang']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=251)\n",
    "\n",
    "    train_df = pd.DataFrame({'cuneiform': X_train, 'lang': y_train})\n",
    "    test_df = pd.DataFrame({'cuneiform': X_test, 'lang': y_test})\n",
    "    train_df.sort_values(by='lang', inplace=True)\n",
    "    test_df.sort_values(by='lang', inplace=True)\n",
    "\n",
    "    train_df.reset_index(drop=True, inplace = True)\n",
    "    test_df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "    print(\"Data has been split!\")\n",
    "\n",
    "    #-----------------------Create a DTM---------------------------------------------\n",
    "\n",
    "    #train the model\n",
    "    tfidf_model = vectorizer.fit(train_df[\"cuneiform\"])\n",
    "\n",
    "    #apply it to the training data\n",
    "    tfidf_matrix_train = vectorizer.transform(train_df[\"cuneiform\"])\n",
    "\n",
    "    #turn results into a document term matrix\n",
    "    dense_matrix_train = tfidf_matrix_train.todense()   \n",
    "\n",
    "    # Retrieve feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names_out() \n",
    "\n",
    "    # Create a DataFrame with the dense matrix and feature names as columns\n",
    "    tfidf_df_train = pd.DataFrame(dense_matrix_train, columns=feature_names)\n",
    "\n",
    "    #Create document term matrix of Test data\n",
    "    tfidf_matrix_test = vectorizer.transform(test_df[\"cuneiform\"])\n",
    "\n",
    "    #turn results into a document term matrix\n",
    "    dense_matrix_test = tfidf_matrix_test.todense()   \n",
    "\n",
    "    # Retrieve feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names_out() \n",
    "\n",
    "    # Create a DataFrame with the dense matrix and feature names as columns\n",
    "    tfidf_df_test = pd.DataFrame(dense_matrix_test, columns=feature_names)\n",
    "\n",
    "    #----------------------- Preprocess and Standardize the Data ----------------------\n",
    "\n",
    "    #Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(tfidf_df_train)\n",
    "    training_df_standardized = pd.DataFrame(scaler.transform(tfidf_df_train), columns=tfidf_df_train.columns) #dataframe of standardized data\n",
    "    test_df_standardized = pd.DataFrame(scaler.transform(tfidf_df_test), columns=tfidf_df_test.columns)\n",
    "\n",
    "    # ---------------------------- Dimension Reduction -------------------------\n",
    "    x_subset_train = training_df_standardized\n",
    "    y_subset_train = train_df[\"lang\"].values\n",
    "\n",
    "    x_subset_test = test_df_standardized.values\n",
    "    y_subset_test = test_df[\"lang\"].values\n",
    "\n",
    "    pca_trained = pca.fit(x_subset_train)\n",
    "    pca_train_results = pca_trained.transform(x_subset_train)\n",
    "    pca_test_results = pca_trained.transform(x_subset_test)\n",
    "\n",
    "    #evaluate the pc variance captured\n",
    "    explained_variance_ratio = pca_trained.explained_variance_ratio_\n",
    "    variance_df = pd.DataFrame(data={\n",
    "        'Principal Component': [f'PC{i+1}' for i in range(len(explained_variance_ratio))],\n",
    "        'Explained Variance' : explained_variance_ratio,\n",
    "        'Cumulative Variance' : explained_variance_ratio.cumsum()\n",
    "    })\n",
    "\n",
    "    return [pca_train_results, pca_test_results, y_subset_train, y_subset_test, variance_df]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split!\n"
     ]
    }
   ],
   "source": [
    "svd_results_outliers_removed = pca_assessment(cuneiform_df, TfidfVectorizer(analyzer='char'), TruncatedSVD(n_components=400, random_state=251))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\keega\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but TruncatedSVD was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svd_results_no_outliers_removed = pca_no_outliers_removed(cuneiform_df, TfidfVectorizer(analyzer='char'), TruncatedSVD(n_components=400, random_state=251))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD With Outlier Removal :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Principal Component</th>\n",
       "      <th>Explained Variance</th>\n",
       "      <th>Cumulative Variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PC1</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.005051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PC2</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.009196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PC3</td>\n",
       "      <td>0.004045</td>\n",
       "      <td>0.013242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PC4</td>\n",
       "      <td>0.003775</td>\n",
       "      <td>0.017016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PC5</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.020620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>PC396</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.874949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>PC397</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.876625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>PC398</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.878292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>PC399</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.879953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>PC400</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.881605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Principal Component  Explained Variance  Cumulative Variance\n",
       "0                   PC1            0.005051             0.005051\n",
       "1                   PC2            0.004145             0.009196\n",
       "2                   PC3            0.004045             0.013242\n",
       "3                   PC4            0.003775             0.017016\n",
       "4                   PC5            0.003603             0.020620\n",
       "..                  ...                 ...                  ...\n",
       "395               PC396            0.001681             0.874949\n",
       "396               PC397            0.001677             0.876625\n",
       "397               PC398            0.001666             0.878292\n",
       "398               PC399            0.001661             0.879953\n",
       "399               PC400            0.001653             0.881605\n",
       "\n",
       "[400 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVD Without Outlier Removal :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Principal Component</th>\n",
       "      <th>Explained Variance</th>\n",
       "      <th>Cumulative Variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PC1</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.005051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PC2</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.009196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PC3</td>\n",
       "      <td>0.004045</td>\n",
       "      <td>0.013242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PC4</td>\n",
       "      <td>0.003775</td>\n",
       "      <td>0.017016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PC5</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.020620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>PC396</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.874949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>PC397</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.876625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>PC398</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.878292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>PC399</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.879953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>PC400</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.881605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Principal Component  Explained Variance  Cumulative Variance\n",
       "0                   PC1            0.005051             0.005051\n",
       "1                   PC2            0.004145             0.009196\n",
       "2                   PC3            0.004045             0.013242\n",
       "3                   PC4            0.003775             0.017016\n",
       "4                   PC5            0.003603             0.020620\n",
       "..                  ...                 ...                  ...\n",
       "395               PC396            0.001681             0.874949\n",
       "396               PC397            0.001677             0.876625\n",
       "397               PC398            0.001666             0.878292\n",
       "398               PC399            0.001661             0.879953\n",
       "399               PC400            0.001653             0.881605\n",
       "\n",
       "[400 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"SVD With Outlier Removal :\")\n",
    "display(svd_results_outliers_removed[4])\n",
    "\n",
    "print(\"\\nSVD Without Outlier Removal :\")\n",
    "display(svd_results_no_outliers_removed[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_func(x_subset, y_subset, model, param_grid, iterations):\n",
    "    #grid = RandomizedSearchCV(model, param_grid, random_state=251, scoring='f1_samples' ,n_iter=iterations, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "    grid = RandomizedSearchCV(model, param_grid, random_state=251, scoring = 'f1_weighted', n_iter=iterations, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "    #fit the model\n",
    "    grid.fit(x_subset, y_subset)\n",
    "\n",
    "    #best parameters\n",
    "    best_params = grid.best_params_\n",
    "\n",
    "    #df of parameters and their r2 scores\n",
    "    param_results = pd.DataFrame(grid.cv_results_)\n",
    "    \n",
    "    return [best_params, param_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gradient_boost_grid = {\n",
    "    'random_state' : [251],\n",
    "    'loss' : ['log_loss'],\n",
    "    'learning_rate' : [0.1],\n",
    "    'max_iter' : [200, 500],\n",
    "    'max_depth' : [100, 200, 500],\n",
    "}\n",
    "#{'random_state': 251, 'max_iter': 200, 'max_depth': 100, 'loss': 'log_loss', 'learning_rate': 0.1}\t best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Results of Gradient Boost Model using TFIDFVectorizer, Outlier Removal, then 400 Component Truncated SVD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'random_state': 251, 'max_iter': 200, 'max_de...</td>\n",
       "      <td>0.746006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'random_state': 251, 'max_iter': 200, 'max_de...</td>\n",
       "      <td>0.746006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'random_state': 251, 'max_iter': 500, 'max_de...</td>\n",
       "      <td>0.746006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'random_state': 251, 'max_iter': 200, 'max_de...</td>\n",
       "      <td>0.746006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'random_state': 251, 'max_iter': 500, 'max_de...</td>\n",
       "      <td>0.746006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              params  mean_test_score  \\\n",
       "0  {'random_state': 251, 'max_iter': 200, 'max_de...         0.746006   \n",
       "1  {'random_state': 251, 'max_iter': 200, 'max_de...         0.746006   \n",
       "2  {'random_state': 251, 'max_iter': 500, 'max_de...         0.746006   \n",
       "3  {'random_state': 251, 'max_iter': 200, 'max_de...         0.746006   \n",
       "4  {'random_state': 251, 'max_iter': 500, 'max_de...         0.746006   \n",
       "\n",
       "   rank_test_score  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters :  {'random_state': 251, 'max_iter': 200, 'max_depth': 100, 'loss': 'log_loss', 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#remove outliers\n",
    "gb_result_outliers_removed = grid_search_func(svd_results_outliers_removed[0], svd_results_outliers_removed[2], HistGradientBoostingClassifier(), gradient_boost_grid, 5)\n",
    "gb_param_df_outliers_removed = gb_result_outliers_removed[1][['params','mean_test_score', 'rank_test_score']].sort_values(by = 'rank_test_score', kind = 'mergesort')\n",
    "\n",
    "print(\"\\nGrid Results of Gradient Boost Model using TFIDFVectorizer, Outlier Removal, then 400 Component Truncated SVD\")\n",
    "display(gb_param_df_outliers_removed)\n",
    "print(\"Best Parameters : \", gb_result_outliers_removed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Results of Gradient Boost Model using TFIDFVectorizer, No Outlier Removal, then 400 Component Truncated SVD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'random_state': 251, 'max_iter': 200, 'max_de...</td>\n",
       "      <td>0.746006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'random_state': 251, 'max_iter': 200, 'max_de...</td>\n",
       "      <td>0.746006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'random_state': 251, 'max_iter': 500, 'max_de...</td>\n",
       "      <td>0.746006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'random_state': 251, 'max_iter': 200, 'max_de...</td>\n",
       "      <td>0.746006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'random_state': 251, 'max_iter': 500, 'max_de...</td>\n",
       "      <td>0.746006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              params  mean_test_score  \\\n",
       "0  {'random_state': 251, 'max_iter': 200, 'max_de...         0.746006   \n",
       "1  {'random_state': 251, 'max_iter': 200, 'max_de...         0.746006   \n",
       "2  {'random_state': 251, 'max_iter': 500, 'max_de...         0.746006   \n",
       "3  {'random_state': 251, 'max_iter': 200, 'max_de...         0.746006   \n",
       "4  {'random_state': 251, 'max_iter': 500, 'max_de...         0.746006   \n",
       "\n",
       "   rank_test_score  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters :  {'random_state': 251, 'max_iter': 200, 'max_depth': 100, 'loss': 'log_loss', 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#keep outliers\n",
    "gb_result_no_outliers_removed = grid_search_func(svd_results_no_outliers_removed[0], svd_results_no_outliers_removed[2], HistGradientBoostingClassifier(), gradient_boost_grid, 5)\n",
    "gb_param_df_no_outliers_removed = gb_result_no_outliers_removed[1][['params','mean_test_score', 'rank_test_score']].sort_values(by = 'rank_test_score', kind = 'mergesort')\n",
    "\n",
    "print(\"\\nGrid Results of Gradient Boost Model using TFIDFVectorizer, No Outlier Removal, then 400 Component Truncated SVD\")\n",
    "display(gb_param_df_no_outliers_removed)\n",
    "print(\"Best Parameters : \", gb_result_no_outliers_removed[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
